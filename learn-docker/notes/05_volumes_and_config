------------------------------------------------------------------
| CHAPTER 5 - DATA VOLUMES & CONFIGURATION                       |
------------------------------------------------------------------

- Creating and Mounting Data Volumes

    - Containers are meant to be stateless.  One way to get around this is to use Docker 
        volumes.  Volumes have a life cycle that goes beyond the life cycle of containers.



- Modifying the Container Layer

    - Before looking at volumes, we first discuss what happens if an application changes 
        something in the filesystem of the container.  In this case, the changes are all
        happening in the writable container layer.

        # Write to the container filesystem
        $ docker container run --name demo \
            alpine /bin/sh -c 'echo "This is a test" > sample.txt'


        # Now we can view the changes
        $ docker container diff demo

        A /sample.txt


    - Now, when we remove the container from memory, the container layer will also be
        removed, with all changes irreversibly deleted.



- Creating Volumes

    - First, we create a new volume.  The named volume can then be mounted into a container
        and used for persistent data storage.  The default driver is the 'local' driver,
        which stores the data locally in the host filesystem.

        # Create new volume
        $ docker volume create sample


    - Now, we can find out where the data is being stored.

        # Inspect new volume
        $ docker volume inspect sample

        [
            {
                "CreatedAt": "2020-04-15T22:06:52-06:00",
                "Driver": "local",
                "Labels": {},
                "Mountpoint": "/var/lib/docker/volumes/sample/_data",
                "Name": "sample",
                "Options": {},
                "Scope": "local"
            }
        ]


    - Since the volume is in a protected part of the filesystem, we'll need to add the 
        --privileged flag to the container.

        # --privileged allows access to the devices of the host
        # --pid=host allows container to access process tree of host

        $ docker run -it --rm --privileged --pid=host fundamentalsofdocker/nsenter


    - And we can access the mount point from the host:

        / # cd /var/lib/docker/volumes/sample/_data
        / # ls -l


    - There are many other volume drivers available from third parties, in the form of plugins.
        Volume drivers include cloud storage, NFS drives, software-defined storage, and more.



- Mounting a Volume

    - Now that we have created a named volume, we can mount it into a container.

        # Run container with volume mounted
        $ docker container run --name test -it -v sample:/data alpine /bin/sh


    - Now, inside the container, we can create files in the /data folder.

        / # cd /data 
        / # echo "Some data" > data.txt 
        / # echo "Some more data" > data2.txt 
        / # exit


    - If we navigate to the folder where the volume was created, we should now see the new
        data files.

        # New data files are present
        $ ls /var/lib/docker/volumes/sample/_data

        # Print file in volume
        $ cd /var/lib/docker/volumes/sample/_data
        $ cat data.txt

        # Add a new file to the volume
        $ echo "This file was created on the host." > host-data.txt


    - Now, we can mount the volume into a new container, and we'll be able to see all the files
        in the volume.

        # Create another container with the same volume mounted
        $ docker container run --name test2 -it -v sample:/data centos:7 /bin/bash

      And we can still see the files we have created:

        / # cd /data
        / # ls -l 



- Removing Volumes

    - Here, we delete the volume we previously created:

        # Delete volume
        $ docker volume rm sample

        # Remove all running containers to clean up the system
        $ docker container rm -f $(docker container ls -aq)



- Sharing Data Between Containers

    - As always when multiple applications or processes concurrently access data, we have to
        be careful to avoid inconsistencies.  To avoid concurrency prolems like race conditions,
        ideally we should only allow one process to write at a time, while others are read-only
        for the duration of the write.

      We can make a volume read-only for a given container to enforce this.


        # Create a writer
        $ docker container run -it --name writer -v shared-data:/data alpine /bin/sh

        # Create a file in the container, should succeed
        # / echo "I can create a file" > /data/sample.txt


        # Create a reader
        $ docker container run -it --name reader -v shared-data:/app/data:ro ubuntu:19.04 /bin/bash

        # Try to create a file, should fail
        # / echo "Try to break read/only" > /app/data/data.txt



- Using Host Volumes

    - Sometimes, often because your legacy application needs access to some folder, it is useful 
        to map a volume directly to a specific host folder.


    - Here, we mount a subdirectory of our current working directory into the container at 
        /app/src.

        # Mount subdirectory of current working directory
        $ docker container run --rm -it -v $(pwd)/src:/app/src alpine:latest /bin/sh


    - Developers tend to use this trick often when working on applications running in containers
        to make sure their most recent code changes are reflected, without the need to rebuild
        and rerun the container after each change.


    - For example, say we want to create a simple static website using nginx as our web server:

        1. First, create a new folder for the host where we will put our web assets.

             $ mkdir ~/my-web
             $ cd ~/my-web


        2. Next, create a simple web page.

             $ echo "<h1>Personal Website</h1>" > index.html


        3. Add a Dockerfile that will contain instructions on how to build the image containing
             our sample website.

             # Dockerfile
             FROM nginx:alpine
             COPY . /usr/share/nginx/html


        4. Build the image.

             $ docker image build -t my-website:1.0 .


        5. Run a container from the image.  Then, navigate to http://localhost:8080 to see the
             web page.

             $ docker container run -d --name my-site -p 8080:80 my-website:1.0


        6. Now, edit the index.html file.

             # Add this to index.html
             <p>This is some text.</p>


        7. If we save it and reload, our changes are unfortunately not reflected.  Host-mounted
             volumes can be used to solve this problem:

             $ docker container rm -f my-site
             $ docker container run -d \
                                    --name my-site \
                                    -v $(pwd):/usr/share/nginx/html \
                                    -p 8080:80 \
                                    my-website:1.0

           Now, if we make changes and reload, our changes will be reflected.



- Defining Volumes in Images

    - Some applications, such as databases running in containers, need to persist their data
        beyond the lifetime of the container.  In this case, they can use volumes.


    - For instance, the maintainers of MongoDB also publish an image to Docker Hub which can 
        be used to run an instance of the database in a container.

      To create volumes when building images, the 'VOLUME' command is used in the Dockerfile.

        # Volume definitions in Dockerfile
        VOLUME /app/data
        VOLUME /app/data, /app/profiles, /app/config
        VOLUME ["/app/data", "/app/profiles", "/app/config"]


    - When a container is started, Docker automatically creates a volume and mounts it to the
        corresponding target folder of the container, for each path defined in the Dockerfile.
        Since each volume is created by Docker automatically, it will have a SHA-256 as its ID.


    - As an example, we'll look at how the MongoDB configures volumes.

        1. Pull the MongoDB image.

             $ docker image pull mongo:3.7


        2. Inspect the volume configuration in the image.

             $ docker image inspect \
                                   --format='{{json .ContainerConfig.Volumes}}' \
                                   mongo:3.7 | jq

             {
                 "/data/configdb": {},
                 "/data/db": {}
             }


        3. Run an instance of MongoDB in the background as a daemon.

            # Run Mongo instance
            $ docker run --name my-mongo -d mongo:3.7


        4. Now, inspect the container to get information about the volumes that were created.

            # Inspect Mongo container
            $ docker inspect --format '{{json .Mounts}}' my-mongo | jq


           In the results, the 'Source' gives us the path to the host directory, where the data
             produced by the MongoDB inside the container will be stored.

            [
              {
                "Type": "volume",
                "Name": "b9ea0158b5...",
                "Source": "/var/lib/docker/volumes/b9ea0158b.../_data",
                "Destination": "/data/configdb",
                "Driver": "local",
                ...
              },
              {
                "Type": "volume",
                "Name": "5becf84b1e...",
                "Source": "/var/lib/docker/volumes/5becf84b1.../_data",
                "Destination": "/data/db",
                ...
              }
            ]



- Configuring Containers

    - We often need to provide some sort of configuration to the application running inside
        the container.  For instance, we could use a configuration to run in development,
        test, staging, and production environments.

      In Linux, configuration values are often provided via environment variables.  


    - Remember, however, that environment variables set on the host will not be available to
        applications running inside of containers.  

        # View environment variables in host
        $ export

        # View environment variables inside container, will be different
        $ docker container run --rm -it alpine /bin/sh
        / # export



- Defining Environment Variables for Containers

    - We can pass configuration values into the container at start time using the '--env'
        option.  

        # Pass in environment variable
        $ docker container run --rm -it \
                               --env LOG_DIR=/var/log/my-log \
                               alpine /bin/sh

        # Inside container
        / # export | grep LOG_DIR

        export LOG_DIR='/var/log/my-log'


    - Note that we can also supply multiple environment variables.

        # Multiple variables
        $ docker container run --rm -it \
                               --env LOG_DIR=/var/log/my-log \
                               --env MAX_LOG_FILES=5 \
                               --env MAX_LOG_SIZE=1G \
                               alpine /bin/sh



- Using Configuration Files

    - Complex applications have many environment variables to configure, and this can make
        the commands to start the containers unwieldy.  Instead, we can pass the list
        of environment variable definitions as a file.

        # File development.config
        LOG_DIR=/var/log/my-log
        MAX_LOG_FILES=5
        MAX_LOG_SIZE=1G

        # Pass file of environment variables
        $ docker container run --rm -it \
                               --env-file ./development.config \
                               alpine sh -c "export"



- Defining Environment Variables in Container Images

    - Sometimes, we want to define some default value for an environment variable that must
        be present in each instance of a given container.  

        # Dockerfile
        FROM alpine:latest
        ENV LOG_DIR=/var/log/my-log
        ENV MAX_LOG_FILES=5
        ENV MAX_LOG_SIZE=1G


        # Build the image
        $ docker image build -t my-alpine .


        # Run the container, variables are present
        $ docker container run --rm -it \
                               my-alpine sh -c "export | grep LOG"

        export LOG_DIR='/var/log/my-log'
        export MAX_LOG_FILES='5'
        export MAX_LOG_SIZE='1G'


    - If we have variables defined in the image, then also pass in values when starting the
        container, the values passed in will override the defaults.

        $ docker container run --rm -it \
                               --env MAX_LOG_SIZE=2G \
                               --env MAX_LOG_FILES=10 \
                               my-alpine sh -c "export | grep LOG"



- Environment Variables at Build Time

    - Sometimes, we might want the possibility to define some environment variables that are
        valid at the time we build a container image.  For instance, we may want to pass 
        arguments into the Dockerfile.

        # Dockerfile takes BASE_IMAGE_VERSION argument

        ARG BASE_IMAGE_VERSION=12.7-stretch
        FROM node:${BASE_IMAGE_VERSION}
        WORKDIR /app
        COPY packages.json .
        RUN npm install
        COPY . .
        CMD npm start


        # Pass argument in at image build time
        $ docker image build \
                             --build-arg BASE_IMAGE_VERSION=12.7-alpine \
                             -t my-node-app-test .