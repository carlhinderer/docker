------------------------------------------------------------------
| CHAPTER 12 - ORCHESTRATORS                                     |
------------------------------------------------------------------

- The Tasks of an Orchestrator

    - Reconciling the Desired State

        - We declaratively specify how we want our application to run, and the 
            orchestrator reconciles the application to our specification.

        - For instance, the orchestrator would:
            - Start the containers to achieve the desired state
            - Monitor the application state over time
            - Scale up the containers if one crashes
            - Scale down the containers if too many are running
            - Kill containers that are running an outdated version of the desired image


    - Replicated and Global Services

        - A 'replicated' service is a service required to run a specific number of
            instances (ie 10 instances).

        - A 'global' service is a service required to have exactly one instance 
            running on each worker node in a cluser.

        - In a cluster, we typically have 2 types of nodes: 'managers' and 'workers'.
            A manager node is used exclusively by the orchestrator to run the cluser
            and does not have any other workload.  Worker nodes run the actual
            applications.

        - In Kubernetes, a global service is also called a 'DaemonSet'.


    - Service Discovery

        - The orchestrator should have total control over which containers are run on
            which nodes, and we shouldn't try to keep track of it.

        - Service A should not have to worry about where to find Service B.  The 
            orchestrator should tell Service A at runtime where to find it.  


    - Routing

        - The orchestrator should take over the task of funneling data packets between
            services.  

        - Data packets might be sent between containers on the same cluster node, on
            containers on different cluster nodes, or to/from outside the cluster
            entirely.  All of these cases are handled by the orchestrator.


    - Load Balancing

        - In a highly available distributed application, all components have to be
            redundant.  If one instance fails, the service as a whole should still be
            operational.

        - Requests for services should be distributed equally to all the instances.  Simple
            round robin is the most frequently used algorithm.

        - We expect the orchestrator to handle load balancing for requests between services
            and requests from external sources.


    - Scaling

        - In real-life scenarios, the workload varies over time.  When scaling up or
            down, we expect orchestrators to distributed the instances in a meaningful
            way.

        - For instance, new instances should not all be provisioned on the same server
            rack, since failure of that rack could take down the entire service.

        - In the cloud, we often use the term 'availability zone' instead of thinking
            about server racks.


    - Self-Healing

        - Orchestrators should be able to monitor the health of cluster nodes, and take
            them out of the scheduler loop if they have crashed.  If it can do this, it
            creates a 'self-healing' system.

        - Note that often, only the application itself knows whether it is healthy or not.
            So, it may need to define an API endpoint that returns whether it is healthy
            or not that the orchestrator can call.


    - Zero Downtime Deployments

        - Rolling updates can be used to avoid downtime during deployments.  We can
            use the orchestrator to do these updates automatically, and to roll
            back if the update is not successful.

        - We deploy the new code, but it is only available internally, and then we can
            run smoke tests to make sure the new version is working as expected.

        - In a 'blue-green deployment', once we have verified that the new verison works,
            we switch over from the old 'blue' version to the new 'green' version.

        - In a 'canary release', we direct a small amount of traffic (maybe 1%) to the
            new version and carefully monitor it.  Then, we slowly increase the percentage
            of traffic going to the new version, continuing to monitor it.


    - Affinity and Location Awareness

        - Sometimes, certain services require specific hardware on the nodes they are
            attached to (ie SSDs or GPUs).  Orchestrators allow us to define these 
            affinities for specific applications, and only nodes that support the 
            affinities will have those applications provisioned to them.

        - Note that to maintain high availability, we should have multiple nodes that
            support the affinities.

        - Some orchestrators support 'location awareness' or 'geo awareness', which allows
            us to spread clusters across multiple geographic areas.  This ensures that
            the application will remain available even if an entire geographic area goes
            down.


    - Security

         - Secure Communication and Cryptographic Node Identity

             - We want to make sure that the cluster is secure.  Only trusted nodes can 
                 join the cluster.  

             - Each node that joins the cluster gets a cryptographic node identity, and all
                 communications between the nodes must be encrypted.  For this, nodes can
                 use MTLS (Mutual Transport Layer Security).

             - In order to authenticate nodes of the cluster with each other, certificates
                 are used.  These certificates are automatically rotated periodically in
                 case a certificate is leaked.

             - The communication that happens in a cluster can be separated into 3 types.

                 1. The Management Plane - This is used by cluster managers to schedule 
                      instances, execute health checks, create and modify volumes, secrets,
                      and networks.

                 2. The Control Plane - This is used to exchange important state information
                      between all nodes of the cluster.  For instance, this is used to 
                      update the local IP tables of clusters.

                 3. The Data Plane - Where actual application services communicate with each
                      other and exchange data.

             - Normally, orchestrators care mainly about securing the management and control 
                 planes.  Securing the data plane is left to the user.


         - Secure Networks and Network Policies

             - Not every service needs to communicate with every other service in the cluster.
                 So, we should only run services in the same networking sandbox that absolutely
                 need to communicate with each other.

             - We can use a SDN (Software-Defined Network) to group application services.

             - Or we can have one flat network and use network policies to control who does and
                 doesn't have access to one particular service or group of services.


         - RBAC

             - An orchestrator must provide RBAC to provide access to the cluster and its
                 resources.  This ensures that unauthorized personnel cannot do harm to the 
                 system.

             - We can implement RBAC with grants, giving specific permissions to users.


         - Secrets

             - Secrets include passwords and API keys or tokens.  In the past, these were
                 hard-coded or stored in plain text in config files.  

             - Orchestrators can encrypt these secrets and store them in the highly available 
                 cluster state database.  So, they are secured at rest.

             - Once a secret is requested by an authorized application service, the secret is
                 sent only to the nodes on which that particular service is running.  The 
                 secret value is never stored on the node, but is mounted into the container
                 in a 'tmpfs' RAM-based volume.  The secret is only available in plain text
                 inside the container.

             - The secrets are sent to the nodes encrypted by MTLS.  So they are secure in 
                 transit also.


         - Content Trust

             - We want to make sure that only trusted images run in our production cluster.
                 To do this, we sign images at the source (ie a CI server) and validate the
                 signatures at the target.


         - Revese Uptime

             - Let's say a hacker does gain root access to one of your cluster nodes.  This is
                 bad enough, but now they could mask thir presence on the node and use it as
                 a basis to attack other nodes in your cluster.

             - To mitigate this, we can leverage the fact that containers are ephemeral and nodes
                 are quickly provisioned.  We can just kill each cluster node after a certain
                 uptime (ie 1 day).  Then, we can replace it with a freshly provisioned node.


    - Introspection

        - The orchestrator should collect system metrics from all the cluster nodes and
            make them accessible to the operators.  Metrics include CPU, memory, and disk
            usage, network bandwidth consumption, and many more.

        - This information should be available on a per-node basis, and also in an 
            aggregate form.  It should be available in dashboards and via API calls.

        - We also need to access logs provided by the containers.

        - We also want to be able to 'exec' into each container if we have the correct
            authorization to do so.

        - In highly distributed applications, tracing a request end-to-end can be very
            difficult.  Ideally, an orchestrator would provide some facility for tracing
            requests.



- Overview of Popular Orchestrators

    - Kubernetes

        - Originally designed at Google, based on lessons learned from their Borg system,
            donated to the CNCF in 2015.

        - Has a significant learning curve for new users.

        - Is the de facto standard for container orchestrators.


    - Docker Swarm

        - Docker's solution for managing clusters of host machines.

        - SwarmKit, first introduced in 2016, is now an integral part of the Docker engine
            itself.

        - Designed to be easy to set up and secure out of the box.


    - Apache Mesos and Marathon

        - Mesos was an open source project, originally designed to make a cluster of
            servers or nodes look like one big server from the outside.  It makes the
            management of computer clusters simple.  It was first released in 2009.

        - Users don't care about individual servers, they just assume they have a big
            pool of resources at their disposal.

        - Mesos is popular mainly in the big data space for services like Spark or Hadoop.

        - Mesos is really the underlying infrastructure for other interesting services
            built on top of it.  Marathon is a container orchestrator built on top of
            Mesos.


    - Amazon ECS

        - ECS is Amazon's AWS-specific orchestrator.  It is highly scalable, fast, and provides
            access to other popular AWS services.  

        - Amazon provides its own private image registry.


    - Microsoft ACS

        - ACS is Microsoft's orchestrator designed to run on Azure.  In light of the
            popularity of Kubernetes, it has been rebranded to 'Azure Kubernetes Service'.