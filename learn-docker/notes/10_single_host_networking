------------------------------------------------------------------
| CHAPTER 10 - SINGLE-HOST NETWORKING                            |
------------------------------------------------------------------

- The Container Network Model

    - In the real world, a containerized business application needs several containers to
        collaborate to achieve a goal.  Docker has a very simple networking model called the
        'CNM' (Container Network Model).

    - There are 3 elements:

        1. Sandbox
             The sandbox perfectly isolates the container, with no inbound connections allowed.

        2. Endpoint
             An endpoint is a controlled gateway into the network's sandbox.  The endpoint 
               connects the network sandbox to the network.

        3. Network
             The network is the pathway that transports the data packets of an instance of 
               communication from endpoint to endpoint (container to container).



- CNM Implementations

    Network                   Company      Scope      Description
    --------------------------------------------------------------------------------------------
    Bridge                    Docker       Local      Simple network based on Linux bridges to allow 
                                                        networking on a single host

    Macvlan                   Docker       Local      Configures multiple layer 2 (that is, MAC) 
                                                        addresses on a single physical host interface

    Overlay                   Docker       Global     Multinode-capable container network based on 
                                                        Virtual Extensible LAN (VXLan)

    Weave Net                 Weaveworks   Global     Simple, resilient, multi-host Docker networking

    Contiv Network Plugin     Cisco        Global     Open source container networking


    - All network types not directly provided by Docker can be added to the Docker host as a plug-in.



- Network Firewalling

    - Docker was built with security in mind.  Software-defined networks are easy and cheap to
        create, yet they perfectly firewall containers that are attached to the network from
        other containers.


    - For instance, if we have 2 sets of services that need to communicate with each other:

        c1 <--> c2      c3 <--> c4
            n1              n2

      c1 and c2 can communicate with each other, but not with c3 or c4.


    - In this case, our productCatalog needs to communicate with both the api and DB.

        webAPI  <-->  productCatalog  <-->  DB
                 n1                    n2

      The productCatalog can communicate with both the webAPI and DB, but they cannot communicate
        with each other.


    - Since creating SDNs is cheap, and each network provides security by isolating resources 
        from unauthorized access, it is highly recommended that we create a separate network for
        each type of communication that needs to occur.



- Working with the Bridge Network

    - The Docker bridge network is the first implementation of the CNM we look at in detail.  It
        is based on the Linux bridge.  


    - When the Docker daemon runs for the first time, it creates a Linux bridge and calls it
        'docker8'.  This is the default behavior and can be changed with configuration.

      Docker then creates a network with this Linux bridge and calls the network 'bridge'.  All
        containers that we create on a Docker host, that we don't bind explicitly to another
        network, are automatically attached to this bridge network.


    - To list all networks on the host:

        # List networks on host
        $ docker network ls



- Inspecting the Bridge Network

    - To get details about a particular network:

        # Get details about bridge network
        $ docker network inspect bridge

        [
            {
                "Name": "bridge",
                "Id": "a0ed2e2a63c4c11b935b70fa15bc1856de6381b27ea7424b3f932e7a55af761b",
                "Created": "2020-04-29T08:37:31.188836369-06:00",
                "Scope": "local",
                "Driver": "bridge",
                "EnableIPv6": false,
                "IPAM": {
                    "Driver": "default",
                    "Options": null,
                    "Config": [
                        {
                            "Subnet": "172.17.0.0/16",
                            "Gateway": "172.17.0.1"
                        }
                    ]
                },
                "Internal": false,
                "Attachable": false,
                "Ingress": false,
                "ConfigFrom": {
                    "Network": ""
                },
                "ConfigOnly": false,
                "Containers": {},
                "Options": {
                    "com.docker.network.bridge.default_bridge": "true",
                    "com.docker.network.bridge.enable_icc": "true",
                    "com.docker.network.bridge.enable_ip_masquerade": "true",
                    "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
                    "com.docker.network.bridge.name": "docker0",
                    "com.docker.network.driver.mtu": "1500"
                },
                "Labels": {}
            }
        ]


    - 'IPAM' (IP Address Management) is software used to track IP addresses that are used on a
        machine.  

      In the IPAM block, we can see the Subnet for the bridge network is set to '172.17.0.0/16', 
        which means that all containers attached to this network will be given an IP address 
        between 172.17.0.2 - 172.17.255.255.  

      The 172.17.0.1 address is reserved for the router of this network, whose role in this type
        of network is taken by the Linux bridge.

      We can expect the first container attached to the network to receive IP address 172.17.0.2,
        and so on.



- Understanding the Bridge Network

    - Network Diagram

        -----------------------------------------------------     --------------------------------
        |  Host Network Namespace                           |     |  Container Network Namespace |
        |                                                   |     |                              |
        |                                                   |     |                              |
        | eth0                  Routing          docker0--------------container                  |
        | 10.10.20.216   <-->   Table     <-->   172.17.0.1 |     |   172.17.0.2                 |
        |                                                   |     |                              |
        -----------------------------------------------------     --------------------------------
           Host eth0            Routing          Linux        veth
                                Table            Bridge       Connection


    - The 'eth0' is a host's NIC.  All traffic to the host comes through eth0.

    
    - The 'Linux bridge' is responsible for routing the network traffic between the host's network
        and the subnet of the bridge network.


    - By default, only egress traffic is allowed, and all ingress is blocked.  So, while container
        applications can reach the internet, they cannot be reached by outside traffic.  Each
        container gets its own virtual ethernet ('vnet') connection with the bridge.


                           eth0
                           10.13.34.23
                               ^
                               |
                               v
                           docker0
                           172.17.0.1
                           /         \
                          /           \
                       veth0         veth1
                       172.17.0.2    172.17.0.2



- Creating a Custom Bridge Network

    - We can define our own custom bridge networks.  Since it is a best practice not to run
        all containers on the same network, we should use additional bridge networks to
        further isolate containers that have no reason to communicate with each other.


    - To create a custom bridge network:

        # Create custom bridge network called 'sample-net'
        $ docker network create --driver bridge sample-net


        # Get the subnet of the new custom network
        $ docker network inspect sample-net | grep Subnet

        "Subnet": "172.18.0.0/16",


    - Use a custom subnet range:

        # Specify a custom subnet range
        $ docker network create --driver bridge --subnet "10.1.0.0/16" test-net



- Attaching Containers to Networks

    - First, we run a container without specifying a network.

        # Create standard container
        $ docker container run --name c1 -it --rm alpine:latest /bin/sh

        # Inspect the new container
        $ docker container inspect c1

        {
            "Gateway": "127.17.0.1",
            "IPAddress": "127.17.0.4",
            "MacAddress": "02:42:ac:11:00:04",
            "Networks": ["bridge": {...}]
        }

        # View container's network information from inside c1 container
        $ ip addr
        $ ip addr show eth0

        # Get information about how requests are routed
        $ ip route


    - Now, run a second container on the same network.

        # Create container c2
        $ docker container run --name c2 -d alpine:latest ping 127.0.0.1

        # Get the IP address
        $ docker container inspect --format "{{.NetworkSettings.IPAddress}}" c2
        172.17.0.3

        # View containers
        $ docker network inspect bridge


    - Create 2 additional containers, this time on a new network.

        # Create containers c3 and c4
        $ docker container run --name c3 -d --network test-net \
                               alpine:latest ping 127.0.0.1

        $ docker container run --name c4 -d --network test-net \
                               alpine:latest ping 127.0.0.1

        # Inspect the new network
        $ docker network inspect test-net


    - Now, we want to see if c3 and c4 can communicate with each other.  

        # Exec into c3
        $ docker container exec -it c3 /bin/sh

        # We can ping c4 successfully
        $ ping c4

        # We can also use the ip address
        $ ping 127.17.0.3


    - A container can be attached to multiple networks.

        # Create container c5
        $ docker container run --name c5 -d \
                               --network sample-net \
                               --network test-net \
                               alpine:latest ping 127.0.0.1



- Removing a Network

    - We can remove a network if there are no containers attached to it.

        # Remove a network
        $ docker network rm test-net
        Error response from daemon: network test-net id 863192... has active endpoints

        # Remove all the containers
        $ docker container rm -f $(docker container ls -aq)

        # Remove the two custom networks that we created
        $ docker network rm sample-net
        $ docker network rm test-net


    - Alternatively, we could remove all the networks that no container is attached to with 
        the prune command:

        # Remove networks without containers
        $ docker network prune --force



- The host and null Network

    - The host Network

        - Normally, for security reasons, we should never attach containers to the host's 
            network.  We might want to do this for very specific reasons, like if we need to 
            analyze network traffic on the host's network.


        - If we do need to do this:

            # Attach container to host's network
            $ docker container run --rm -it --network host alpine:latest /bin/sh

            # Get host network information from inside the container
            $ ip addr show eth0
            $ ip route



    - The null Network

        - Sometimes, we have tasks that don't need any network at all to execute their task.
            If we run them using the 'none' network, they will be isolated from outside access.

            # Run without network
            $ docker container run --rm -it --network none alpine:latest /bin/sh

            # Returns nothing from inside the container
            $ ip addr show eth0
            $ ip route



- Running in an Existing Network Namespace

    - Normally, Docker creates a new network namespace for each container we run.  

      A 'network namespace' corresponds to the sandbox of the CNM.  We can run multiple
        containers in the same network namespace in specific conditions.


    - For instance, this may be useful to debug a running container without running additional
        processes inside the container.

        # Create a new bridge network
        $ docker network create --driver bridge test-net

        # Run a container attached to the network
        $ docker container run --name web -d \
                               --network test-net nginx:alpine

        # Run another container and attach it to our 'web' container
        $ docker container run -it --rm --network container:web \
                               alpine:latest /bin/sh

        # Now, since we're in the same network namespace, we can access nginx on localhost
        $ wget -qO - localhost

        # Clean up container and network
        $ docker container rm --force web
        $ docker network rm test-net



- Managing Container Ports

    - We still need to be able to expose applications to the outside world.  For instance,
        we may have a container running a web server hosting our webAPI from before.

      We want customers to be able to access the service, so we need to open up a port in the
        container that maps to a port on the host.


    - We map a container port to a host port when creating a container.  Here, we start an nginx
        server, which is listening on port 80.  With the -P parameter, we can map all exposed
        container ports to a free port in the 32xxx range.

        # Let Docker pick a port
        $ docker container run --name web -P -d nginx:alpine


        # Check which port is being used
        $ docker container port web
        80/tcp -> 0.0.0.0:32768

        # Can also get the port this way
        $ docker container inspect web | grep HostPort
        32768

        # Or this way
        $ docker container ls


    - To map a container port to a specific host port, we can use the -p argument.

        # Map host 8080 to container 80
        $ docker container run --name web2 -p 8080:80 -d nginx:alpine



- HTTP-Level Routing Using a Reverse Proxy

    - We need to containerize a monolithic application that has evolved over the years into
        an unmaintainable monster.  However, we don't want to break any part of the public
        API.


    - Containerizing the Monolith

        1. First, we'll run the existing application, which uses Flask and is implemented in 
             Python 3.7.

             # Run the application
             $ cd monolith
             $ pip install -r requirements.txt
             $ export FLASK_APP=main.py
             $ flask run


        2. Now, we can use curl to test the app.

             # Test existing application
             $ curl localhost:5000/catalog?category=bicycles


        3. Next, we'll create an entry in the /etc/hosts file that maps 'acme.com' to locahost.

             # Add to /etc/hosts
             127.0.0.1 acme.com

             # Ping name to make sure it works
             $ ping acme.com


        4. Now, we can start containerizing the application.  We need to make sure we have the
             application web server listening on 0.0.0.0 instead of localhost.

             # File: main.py
             if __name__ == '__main__':
                 app.run(host='0.0.0.0' port=5000)


        5. Now, we add a Dockerfile to the 'monolith' folder.

             # File: Dockerfile

             FROM python:3.7-alpine
             WORKDIR /app
             COPY requirements.txt ./
             RUN pip install -r requirements.txt
             COPY . .
             EXPOSE 5000
             CMD python main.py


        6. Build the image for the application.

             # Build image
             $ docker image build -t acme/eshop:1.0 .


        7. Now, we can run the application.

             # Run the containerized application
             $ docker container run --rm -it --name eshop \
                                    -p 5000:5000 \
                                    acme/eshop:1.0

             # Test it, works the same as before
             $ curl http://acme.com:5000/checkout


    - Extracting the First Microservice

        - Now, we'll extract the catalog part of the application into it's own service.  
            We'll implement it using Node.js.  

            # Build the Docker image for the new microservice
            $ docker image build -t acme/catalog:1.0 .

            # Run the container
            $ docker run --rm -it --name catalog -p 3000:3000 acme/catalog:1.0

            # Test the new service
            $ curl http://acme.com:3000/catalog?type=bicycle



    - Using Traefik to Route Traffic

        - Notice that with the original monolith, we accessed it at:

            localhost:5000/catalog?category=bicycles

          While we accessed the containerized version at:

            http://acme.com:3000/catalog?type=bicycle

          We want existing clients to be able to keep using port 5000 so that they don't have
            to be changed.  We will use Traefik, a cloud-native edge router, to manage that
            for us.


        - Traefik is open-source, has a nice web UI for managing and monitoring routes, and can
            be easily combined with Docker.


        - First, we'll run the catalog service.

            $ docker container run --rm -d \
               --name catalog \
               --label traefik.enable=true \
               --label traefik.port=3000 \
               --label traefik.priority=10 \
            --label traefik.http.routers.catalog.rule="Host(\"acme.com\") && PathPrefix(\"/catalog\")" \
               acme/catalog:1.0


               traefik.enable    # Whether this container should be included in the routing
               treafik.port      # Router should forward calls to port 3000
               treafik.priority  # Give the route high priority
               treafik.http.routers.catalog.rule      # Request must begin with 'acme.com/catalog' to
                                                          be routed by this service


        - Now, we can run the 'eshop' container.  We'll forward traffic to port 5000, and we'll set
            priority to low.  This way, URLs starting with 'catalog' will be filtered to new service,
            while other urls will go to the eshop service.

            $ docker container run --rm -d \
                                   --name eshop \
                                   --label traefik.enable=true \
                                   --label traefik.port=5000 \
                                   --label traefik.priority=1 \
                                   --label traefik.http.routers.eshop.rule="Host(\"acme.com\")" \
                                   acme/eshop:1.0


        - Finally, we'll run Traefik as the edge router that will serve as a reverse proxy in front
            of our application.  We mount the Docker socket into the container so that Traefik can
            interact with the Docker engine.  The web UI for Traefik is at port 8080.

            $ docker run -d \
                         --name traefik \
                         -p 8080:8080 \
                         -p 80:80 \
                         -v /var/run/docker.sock:/var/run/docker.sock \
                         traefik:v2.0 --api.insecure=true --providers.docker


        - Now, we can test the URLs to make sure they go to the correct destination.

            $ curl http://acme.com/catalog?type=bicycles
            $ curl http://acme.com/checkout


        - To clean up, we should stop all containers.

            $ docker container rm -f traefik eshop catalog