-----------------------------------------------------------
CHAPTER 10 - SINGLE-HOST NETWORKING
-----------------------------------------------------------

- The Container Network Model

    - In the real world, a containerized business application needs several containers to
        collaborate to achieve a goal.  Docker has a very simple networking model called the
        'CNM' (Container Network Model).

    - There are 3 elements:

        1. Sandbox
             The sandbox perfectly isolates the container, with no inbound connections allowed.

        2. Endpoint
             An endpoint is a controlled gateway into the network's sandbox.  The endpoint 
               connects the network sandbox to the network.

        3. Network
             The network is the pathway that transports the data packets of an instance of 
               communication from endpoint to endpoint (container to container).



- CNM Implementations

    Network                   Company      Scope      Description
    --------------------------------------------------------------------------------------------
    Bridge                    Docker       Local      Simple network based on Linux bridges to allow 
                                                        networking on a single host

    Macvlan                   Docker       Local      Configures multiple layer 2 (that is, MAC) 
                                                        addresses on a single physical host interface

    Overlay                   Docker       Global     Multinode-capable container network based on 
                                                        Virtual Extensible LAN (VXLan)

    Weave Net                 Weaveworks   Global     Simple, resilient, multi-host Docker networking

    Contiv Network Plugin     Cisco        Global     Open source container networking


    - All network types not directly provided by Docker can be added to the Docker host as a plug-in.



- Network Firewalling

    - Docker was built with security in mind.  Software-defined networks are easy and cheap to
        create, yet they perfectly firewall containers that are attached to the network from
        other containers.


    - For instance, if we have 2 sets of services that need to communicate with each other:

        c1 <--> c2      c3 <--> c4
            n1              n2

      c1 and c2 can communicate with each other, but not with c3 or c4.


    - In this case, our productCatalog needs to communicate with both the api and DB.

        webAPI  <-->  productCatalog  <-->  DB
                 n1                    n2

      The productCatalog can communicate with both the webAPI and DB, but they cannot communicate
        with each other.


    - Since creating SDNs is cheap, and each network provides security by isolating resources 
        from unauthorized access, it is highly recommended that we create a separate network for
        each type of communication that needs to occur.



- Working with the Bridge Network

    - The Docker bridge network is the first implementation of the CNM we look at in detail.  It
        is based on the Linux bridge.  


    - When the Docker daemon runs for the first time, it creates a Linux bridge and calls it
        'docker8'.  This is the default behavior and can be changed with configuration.

      Docker then creates a network with this Linux bridge and calls the network 'bridge'.  All
        containers that we create on a Docker host, that we don't bind explicitly to another
        network, are automatically attached to this bridge network.


    - To list all networks on the host:

        # List networks on host
        $ docker network ls



- Inspecting the Bridge Network

    - To get details about a particular network:

        # Get details about bridge network
        $ docker network inspect bridge

        [
            {
                "Name": "bridge",
                "Id": "a0ed2e2a63c4c11b935b70fa15bc1856de6381b27ea7424b3f932e7a55af761b",
                "Created": "2020-04-29T08:37:31.188836369-06:00",
                "Scope": "local",
                "Driver": "bridge",
                "EnableIPv6": false,
                "IPAM": {
                    "Driver": "default",
                    "Options": null,
                    "Config": [
                        {
                            "Subnet": "172.17.0.0/16",
                            "Gateway": "172.17.0.1"
                        }
                    ]
                },
                "Internal": false,
                "Attachable": false,
                "Ingress": false,
                "ConfigFrom": {
                    "Network": ""
                },
                "ConfigOnly": false,
                "Containers": {},
                "Options": {
                    "com.docker.network.bridge.default_bridge": "true",
                    "com.docker.network.bridge.enable_icc": "true",
                    "com.docker.network.bridge.enable_ip_masquerade": "true",
                    "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
                    "com.docker.network.bridge.name": "docker0",
                    "com.docker.network.driver.mtu": "1500"
                },
                "Labels": {}
            }
        ]


    - 'IPAM' (IP Address Management) is software used to track IP addresses that are used on a
        machine.  

      In the IPAM block, we can see the Subnet for the bridge network is set to '172.17.0.0/16', 
        which means that all containers attached to this network will be given an IP address 
        between 172.17.0.2 - 172.17.255.255.  

      The 172.17.0.1 address is reserved for the router of this network, whose role in this type
        of network is taken by the Linux bridge.

      We can expect the first container attached to the network to receive IP address 172.17.0.2,
        and so on.



- Understanding the Bridge Network

    - Network Diagram

        -----------------------------------------------------     --------------------------------
        |  Host Newtork Namespace                           |     |  Container Network Namespace |
        |                                                   |     |                              |
        |                                                   |     |                              |
        | eth0                  Routing          docker0--------------container                  |
        | 10.10.20.216   <-->   Table     <-->   172.17.0.1 |     |   172.17.0.2                 |
        |                                                   |     |                              |
        -----------------------------------------------------     --------------------------------
           Host eth0            Routing          Linux        veth
                                Table            Bridge       Connection


    - The 'eth0' is a host's NIC.  All traffic to the host comes through eth0.

    
    - The 'Linux bridge' is responsible for routing the network traffic between the host's network
        and the subnet of the bridge network.


    - By default, only egress traffic is allowed, and all ingress is blocked.  So, while container
        applications can reach the internet, they cannot be reached by outside traffic.  Each
        container gets its own virtual ethernet ('vnet') connection with the bridge.


                           eth0
                           10.13.34.23
                               ^
                               |
                               v
                           docker0
                           172.17.0.1
                           /         \
                          /           \
                       veth0         veth1
                       172.17.0.2    172.17.0.2



- Creating a Custom Bridge Network

    - We can define our own custom bridge networks.  Since it is a best practice not to run
        all containers on the same network, we should use additional bridge networks to
        further isolate containers that have no reason to communicate with each other.


    - To create a custom bridge network:

        # Create custom bridge network called 'sample-net'
        $ docker network create --driver bridge sample-net


        # Get the subnet of the new custom network
        $ docker network inspect sample-net | grep Subnet

        "Subnet": "172.18.0.0/16",


    - Use a custom subnet range:

        # Specify a custom subnet range
        $ docker network create --driver bridge --subnet "10.1.0.0/16" test-net



- Attaching Containers to Networks

    - First, we run a container without specifying a network.

        # Create standard container
        $ docker container run --name c1 -it --rm alpine:latest /bin/sh

        # Inspect the new container
        $ docker container inspect c1

        {
            "Gateway": "127.17.0.1",
            "IPAddress": "127.17.0.4",
            "MacAddress": "02:42:ac:11:00:04",
            "Networks": ["bridge": {...}]
        }

        # View container's network information from inside c1 container
        $ ip addr
        $ ip addr show eth0

        # Get information about how requests are routed
        $ ip route


    - Now, run a second container on the same network.

        # Create container c2
        $ docker container run --name c2 -d alpine:latest ping 127.0.0.1

        # Get the IP address
        $ docker container inspect --format "{{.NetworkSettings.IPAddress}}" c2
        172.17.0.3

        # View containers
        $ docker network inspect bridge


    - Create 2 additional containers, this time on a new network.

        # Create containers c3 and c4
        $ docker container run --name c3 -d --network test-net \
                               alpine:latest ping 127.0.0.1

        $ docker container run --name c4 -d --network test-net \
                               alpine:latest ping 127.0.0.1

        # Inspect the new network
        $ docker network inspect test-net


    - Now, we want to see if c3 and c4 can communicate with each other.  

        # Exec into c3
        $ docker container exec -it c3 /bin/sh

        # We can ping c4 successfully
        $ ping c4

        # We can also use the ip address
        $ ping 127.17.0.3


    - A container can be attached to multiple networks.

        # Create container c5
        $ docker container run --name c5 -d \
                               --network sample-net \
                               --network test-net \
                               alpine:latest ping 127.0.0.1



- Removing a Network

    - We can remove a network if there are no containers attached to it.

        # Remove a network
        $ docker network rm test-net
        Error response from daemon: network test-net id 863192... has active endpoints

        # Remove all the containers
        $ docker container rm -f $(docker container ls -aq)

        # Remove the two custom networks that we created
        $ docker network rm sample-net
        $ docker network rm test-net


    - Alternatively, we could remove all the networks that no container is attached to with 
        the prune command:

        # Remove networks without containers
        $ docker network prune --force



- The host and null Network

    - The host Network

        - Normally, for security reasons, we should never attach containers to the host's 
            network.  We might want to do this for very specific reasons, like if we need to 
            analyze network traffic on the host's network.


        - If we do need to do this:

            # Attach container to host's network
            $ docker container run --rm -it --network host alpine:latest /bin/sh

            # Get host network information from inside the container
            $ ip addr show eth0
            $ ip route



    - The null Network

        - Sometimes, we have tasks that don't need any network at all to execute their task.
            If we run them using the 'none' network, they will be isolated from outside access.

            # Run without network
            $ docker container run --rm -it --network none alpine:latest /bin/sh

            # Returns nothing from inside the container
            $ ip addr show eth0
            $ ip route



- Running in an Existing Network Namespace

    - Normally, Docker creates a new network namespace for each container we run.  

      A 'network namespace' corresponds to the sandbox of the CNM.  We can run multiple
        containers in the same network namespace in specific conditions.


    - For instance, this may be useful to debug a running container without running additional
        processes inside the container.

        # Create a new bridge network
        $ docker network create --driver bridge test-net

        # Run a container attached to the network
        $ docker container run --name web -d \
                               --network test-net nginx:alpine

        # Run another container and attach it to our 'web' container
        $ docker container run -it --rm --network container:web \
                               alpine:latest /bin/sh

        # Now, since we're in the same network namespace, we can access nginx on localhost
        $ wget -q0 - localhost

        # Clean up container and network
        $ docker container rm --force web
        $ docker network rm test-net



- Managing Container Ports

    - We still need to be able to expose applications to the outside world.  For instance,
        we may have a container running a web server hosting our webAPI from before.

      We want customers to be able to access the service, so we need to open up a port in the
        container that maps to a port on the host.


    - We map a container port to a host port when creating a container.  Here, we start an nginx
        server, which is listening on port 80.  With the -P parameter, we can map all exposed
        container ports to a free port in the 32xxx range.

        # Let Docker pick a port
        $ docker container run --name web -P -d nginx:alpine


        # Check which port is being used
        $ docker container port web
        80/tcp -> 0.0.0.0:32768

        # Can also get the port this way
        $ docker container inspect web | grep HostPort
        32768

        # Or this way
        $ docker container ls


    - To map a container port to a specific host port, we can use the -p argument.

        # Map host 8080 to container 80
        $ docker container run --name web2 -p 8080:80 -d nginx:alpine



- HTTP-Level Routing Using a Reverse Proxy

    - Extracting the First Microservice
    - Using Traefik to Route Traffic
