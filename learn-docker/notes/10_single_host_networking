-----------------------------------------------------------
CHAPTER 10 - SINGLE-HOST NETWORKING
-----------------------------------------------------------

- The Container Network Model

    - In the real world, a containerized business application needs several containers to
        collaborate to achieve a goal.  Docker has a very simple networking model called the
        'CNM' (Container Network Model).

    - There are 3 elements:

        1. Sandbox
             The sandbox perfectly isolates the container, with no inbound connections allowed.

        2. Endpoint
             An endpoint is a controlled gateway into the network's sandbox.  The endpoint 
               connects the network sandbox to the network.

        3. Network
             The network is the pathway that transports the data packets of an instance of 
               communication from endpoint to endpoint (container to container).



- CNM Implementations

    Network                   Company      Scope      Description
    --------------------------------------------------------------------------------------------
    Bridge                    Docker       Local      Simple network based on Linux bridges to allow 
                                                        networking on a single host

    Macvlan                   Docker       Local      Configures multiple layer 2 (that is, MAC) 
                                                        addresses on a single physical host interface

    Overlay                   Docker       Global     Multinode-capable container network based on 
                                                        Virtual Extensible LAN (VXLan)

    Weave Net                 Weaveworks   Global     Simple, resilient, multi-host Docker networking

    Contiv Network Plugin     Cisco        Global     Open source container networking


    - All network types not directly provided by Docker can be added to the Docker host as a plug-in.



- Network Firewalling

    - Docker was built with security in mind.  Software-defined networks are easy and cheap to
        create, yet they perfectly firewall containers that are attached to the network from
        other containers.


    - For instance, if we have 2 sets of services that need to communicate with each other:

        c1 <--> c2      c3 <--> c4
            n1              n2

      c1 and c2 can communicate with each other, but not with c3 or c4.


    - In this case, our productCatalog needs to communicate with both the api and DB.

        webAPI  <-->  productCatalog  <-->  DB
                 n1                    n2

      The productCatalog can communicate with both the webAPI and DB, but they cannot communicate
        with each other.


    - Since creating SDNs is cheap, and each network provides security by isolating resources 
        from unauthorized access, it is highly recommended that we create a separate network for
        each type of communication that needs to occur.



- Working with the Bridge Network

    - The Docker bridge network is the first implementation of the CNM we look at in detail.  It
        is based on the Linux bridge.  


    - When the Docker daemon runs for the first time, it creates a Linux bridge and calls it
        'docker8'.  This is the default behavior and can be changed with configuration.

      Docker then creates a network with this Linux bridge and calls the network 'bridge'.  All
        containers that we create on a Docker host, that we don't bind explicitly to another
        network, are automatically attached to this bridge network.


    - To list all networks on the host:

        # List networks on host
        $ docker network ls



- Inspecting the Bridge Network

    - To get details about a particular network:

        # Get details about bridge network
        $ docker network inspect bridge

        [
            {
                "Name": "bridge",
                "Id": "a0ed2e2a63c4c11b935b70fa15bc1856de6381b27ea7424b3f932e7a55af761b",
                "Created": "2020-04-29T08:37:31.188836369-06:00",
                "Scope": "local",
                "Driver": "bridge",
                "EnableIPv6": false,
                "IPAM": {
                    "Driver": "default",
                    "Options": null,
                    "Config": [
                        {
                            "Subnet": "172.17.0.0/16",
                            "Gateway": "172.17.0.1"
                        }
                    ]
                },
                "Internal": false,
                "Attachable": false,
                "Ingress": false,
                "ConfigFrom": {
                    "Network": ""
                },
                "ConfigOnly": false,
                "Containers": {},
                "Options": {
                    "com.docker.network.bridge.default_bridge": "true",
                    "com.docker.network.bridge.enable_icc": "true",
                    "com.docker.network.bridge.enable_ip_masquerade": "true",
                    "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
                    "com.docker.network.bridge.name": "docker0",
                    "com.docker.network.driver.mtu": "1500"
                },
                "Labels": {}
            }
        ]


    - 'IPAM' (IP Address Management) is software used to track IP addresses that are used on a
        machine.  

      In the IPAM block, we can see the Subnet for the bridge network is set to '172.17.0.0/16', 
        which means that all containers attached to this network will be given an IP address 
        between 172.17.0.2 - 172.17.255.255.  

      The 172.17.0.1 address is reserved for the router of this network, whose role in this type
        of network is taken by the Linux bridge.

      We can expect the first container attached to the network to receive IP address 172.17.0.2,
        and so on.



- Understanding the Bridge Network

    - Network Diagram

        -----------------------------------------------------     --------------------------------
        |  Host Newtork Namespace                           |     |  Container Network Namespace |
        |                                                   |     |                              |
        |                                                   |     |                              |
        | eth0                  Routing          docker0--------------container                  |
        | 10.10.20.216   <-->   Table     <-->   172.17.0.1 |     |   172.17.0.2                 |
        |                                                   |     |                              |
        -----------------------------------------------------     --------------------------------
           Host eth0            Routing          Linux        veth
                                Table            Bridge       Connection


    - The 'eth0' is a host's NIC.  All traffic to the host comes through eth0.

    
    - The 'Linux bridge' is responsible for routing the network traffic between the host's network
        and the subnet of the bridge network.


    - By default, only egress traffic is allowed, and all ingress is blocked.  So, while container
        applications can reach the internet, they cannot be reached by outside traffic.  Each
        container gets its own virtual ethernet ('vnet') connection with the bridge.


                           eth0
                           10.13.34.23
                               ^
                               |
                               v
                           docker0
                           172.17.0.1
                           /         \
                          /           \
                       veth0         veth1
                       172.17.0.2    172.17.0.2



- Creating a Custom Bridge Network

    - We can define our own custom bridge networks.  Since it is a best practice not to run
        all containers on the same network, we should use additional bridge networks to
        further isolate containers that have no reason to communicate with each other.


    - To create a custom bridge network:

        # Create custom bridge network called 'sample-net'
        $ docker network create --driver bridge sample-net


        # Get the subnet of the new custom network
        $ docker network inspect sample-net | grep Subnet

        "Subnet": "172.18.0.0/16",


    - Use a custom subnet range:

        # Specify a custom subnet range
        $ docker network create --driver bridge --subnet "10.1.0.0/16" test-net



- The host and null Network

    - The host Network
    - The null Network


- Running in an Existing Network Namespace


- Managing Container Ports


- HTTP-Level Routing Using a Reverse Proxy

    - Containerizing the Monolith
    - Extracting the First Microservice
    - Using Traefik to Route Traffic