------------------------------------------------------------------
| CHAPTER 7 - USING DOCKER TO SUPERCHARGE AUTOMATION             |
------------------------------------------------------------------

- Executing Simple Admin Tasks in a Container

    - Let's assume we have a handy Perl script to strip all whitespace from a file.

        $ cat sample.txt | perl -lpe 's/^\s*//'


    - The only problem is that we don't have Perl installed on our machine.  But, we can
        use Docker to circumvent the need to install it.

        1. Create a new folder

             $ mkdir -p ~/fod/ch07/simple-task && cd ~/fod/ch07/simple-task


        2. Create a sample file in the new folder

             # File: sample.txt

             1234567890
               This is some text
                another line of text
              more text
                  final line


        3. Now, we can run a container with Perl installed in it.  We use the slim version
             of the official Perl image.

           We map the current working directory on our host to /usr/src/app in the container,
             and set /usr/src/app to be the container's working directory.


             # Run Perl script on our files without installing Perl
             $ docker container run --rm -it \
                                    -v $(pwd):/usr/src/app \
                                    -w /usr/src/app \
                                    perl:slim sh -c "cat sample.txt | perl -lpe 's/^\s*//'"


    - Note that we could even use some older version of Perl if we needed to.  

        perl:some-old-version


    - On hosts that don't have Python 3 installed, we can use this same technique to run
        Python 3 scripts.

        python:3.7.4-alpine python stats.py sample.txt



- Using Test Containers

    - Typical Test Types:

        1. Unit Tests
             Assert the correctness and quality of an individual, isolated piece of the 
               overall application.  

        2. Integration Tests
             Makes sure that pieces that are closely related work together as expected.

        3. Stress and Load Tests
             Tests the behavior of the application when handling lots of current requests
               and large amounts of data.

        4. End-to-End Tests
             Simulate a real user working with the application.


    - The code or component under test is often called a SUT (System Under Test).

      Unit tests are tightly coupled to the actual code, so they live in the same container
        as the SUT.  Integration, stress/load, and end-to-end tests use the public interface
        of the application, so they often run from a separate container.


    - End-to-end tests often use the Selenium web driver to automate actions on a given web
        page.



- Integration Testing a Node.js Application - Preparing the Database

    1. Make a folder and subfolders for our application.  The subfolders are 'tests', 'api',
         and 'database'.

        $ mkdir ~/fod/ch07/integration-test-node && cd ~/fod/ch07/integration-test-node
        $ mkdir tests api database


    2. Create the script to initialize the database.  We will use it to create the Postgres
         database.

        # File: integration-test-node/database/init-script.sql

        CREATE TABLE hobbies(
           hobby_id serial PRIMARY KEY,
           hobby VARCHAR (255) UNIQUE NOT NULL
        );
        
        insert into hobbies(hobby) values('swimming');
        insert into hobbies(hobby) values('diving');
        insert into hobbies(hobby) values('jogging');
        insert into hobbies(hobby) values('dancing');
        insert into hobbies(hobby) values('cooking');


    3. Create a volume where the database will store it's files.

        $ docker volume create pg-data


    4. Run the database container.

        $ docker container run -d \
                               --name postgres \
                               -p 5432:5432 \
                               -v $(pwd)/database:/docker-entrypoint-initdb.d \
                               -v pg-data:/var/lib/postgresql/data \
                               -e POSTGRES_USER=dbuser \
                               -e POSTGRES_DB=sample-db \
                               postgres:11.5-alpine


    5. Verify that the new container is running.  We will see that the initialization
         script was run.

         $ docker container logs postgres

         ...
         server started
         CREATE DATABASE
         
         /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init-db.sql
         CREATE TABLE
         INSERT 0 1
         INSERT 0 1
         INSERT 0 1
         INSERT 0 1
         INSERT 0 1
         
         ...
         
         PostgreSQL init process complete; ready for start up.
         
         2019-09-07 17:22:30.056 UTC [1] LOG: listening on IPv4 address "0.0.0.0", port 5432
         ...



- Integration Testing a Node.js Application - Building the API

    1. Initialize the API project.

        $ cd api
        $ npm init


    2. Add a start command to the package.json.

        # File: package.json

        "scripts": {
          "start": "node index.js",
          "test": "echo \"Error: no test specified\" && exit 1"
        },


    3. Add Express JS.

        $ npm install express --save


    4. Create a 'server.js' file in the 'api' folder.

        # File: api/server.js

        const express = require('express');
        const app = express();

        app.listen(3000, '0.0.0.0', () => {
            console.log('Application listening at 0.0.0.0:3000');
        })

        app.get('/', (req, res) => {
            res.send('Sample API');
        })


    5. Start the app and test the home endpoint.

        # Start the app
        $ npm start

        # Test the home endpoint
        $ curl localhost:3000



- Integration Testing a Node.js Application - Testing the API

    1. Start the test project.

        $ cd tests
        $ npm init


    2. Install and initialize jasmine.

        # Install jasmine
        $ npm install --save-dev jasmine

        # Initialize jasmine for project
        $ node node_modules/jasmine/bin/jasmine init


    3. Now, create the config settings for the 'jasmine.json' file if one wasn't created
         automatically.

        # File: /spec/support/jasmine.json

        {
          "spec_dir": "spec",
          "spec_files": [
            "**/*[sS]pec.js"
          ],
          "stopSpecOnExpectationFailure": false,
          "random": false
        }


    4. Install the 'request' library so that we can make API calls to the SUT.

        $ npm install request --save-dev


    5. Add an 'api-spec.js' file to the 'spec' subfolder of the project.

        # File: tests/spec/api-spec.js

        var request = require("request");

        const base_url = process.env.BASE_URL || 'http://localhost:3000';

        describe("API test suite", () => {
            describe("GET /", () => {
                it("returns status code 200", function(done) {
                    request.get(base_url, (error, response, body) => {
                        expect(response.statusCode).toBe(200);
                        done();
                    });
                });
                it("returns description", function(done) {
                    request.get(base_url, (error, response, body) => {
                        expect(body).toBe("Sample API");
                        done();
                    });
                });
            });
        });


    6. Now, we have to edit the 'package.json' to change the test command to 'jasmine'.

        # File: package.json

        "scripts": {
            "test": "jasmine"
        },


    7. Now, we can run the tests and they should pass.

        $ npm test


    8. Now, to clean up, we can stop the api and remove the Postgres container.



- Integration Testing a Node.js Application - Run Everything in Containers

    1. First, we'll add a Dockerfile for the API.

        # File: api/Dockerfile

        FROM node:alpine
        WORKDIR /usr/src/app
        COPY package.json ./
        RUN npm install
        COPY . .
        EXPOSE 3000
        CMD npm start


    2. Next, add a Dockerfile for the tests.

        # File tests/Dockerfile

        FROM node:alpine
        WORKDIR /usr/src/app
        COPY package.json ./
        RUN npm install
        COPY . .
        CMD npm test


    3. Now, we're ready to run all 3 containers in the right sequence.  We'll create a 
         shell script to do it.

         # File: integration-test-node/test.sh

         docker image build -t api-node api
         docker image build -t tests-node tests
         
         docker network create test-net
         
         docker container run --rm -d \
             --name postgres \
             --net test-net \
             -v $(pwd)/database:/docker-entrypoint-initdb.d \
             -v pg-data:/var/lib/postgresql/data \
             -e POSTGRES_USER=dbuser \
             -e POSTGRES_DB=sample-db \
             postgres:11.5-alpine
         
         docker container run --rm -d \
             --name api \
             --net test-net \
             api-node
         
         echo "Sleeping for 5 sec..."
         sleep 5
         
         docker container run --rm -it \
             --name tests \
             --net test-net \
             -e BASE_URL="http://api:3000" \
             tests-node


    4. Make the script executable.

         $ chmod +x ./test.sh


    5. Now, we can run it.

         $ ./test.sh


    6. After running it, we can run a cleanup script.

         # File: integration-test-node/cleanup.sh
         docker container rm -f postgres api
         docker network rm test-net
         docker volume rm pg-data

         # Make script executable and run it
         $ chmod +x cleanup.sh
         $ ./test.sh



- The Testcontainers Project

    - Testcontainers is a useful project for Java developers.  

      It is a Java library that supports JUnit tests, throwaway instances of common 
        databases, Selenium web browsers, or anything else that can run in a Docker container.



- Using Docker to Power a CI/CD Pipeline - Jenkins Basics

    - In this example, we use Jenkins as our automation server to build a CI/CD pipeline.  
        Other automation servers such as TeamCity work equally well.  

      When using Jenkins, the central document is the 'Jenkinsfile', which will contain the
        definition of our pipeline with its multiple stages.


    - A simple Jenkinsfile with the 'Build', 'Test', 'Deploy to Staging', and 'Deploy to 
        Production' stages might look like this:

        # File: Jenkinsfile

        pipeline {
            agent any
            options {
                skipStagesAfterUnstable()
            }
            stages {
                stage('Build') {
                    steps {
                        echo 'Building'
                    }
                }
                stage('Test') {
                    steps {
                        echo 'Testing'
                    }
                }
                stage('Deploy to Staging') {
                    steps {
                        echo 'Deploying to Staging'
                    }
                }
                stage('Deploy to Production') {
                    steps {
                        echo 'Deploying to Production'
                    }
                }
            }
        }



- Using Docker to Power a CI/CD Pipeline - Creating the Pipeline

    1. First, we create a project folder named 'jenkins-pipeline' and navigate to it.  

         $ mkdir ~/fod/ch07/jenkins-pipeline && cd ~/fod/ch07/jenkins-pipeline


    2. Now, let's create a new volume and run Jenkins in a Docker container.

         $ docker volume create jenkins-data

         $ docker run --rm -d \
                      --name jenkins \
                      -u root \
                      -p 8080:8080 \
                      -v jenkins-data:/var/jenkins_home \
                      -v /var/run/docker.sock:/var/run/docker.sock \
                      -v "$HOME":/home \
                      jenkinsci/blueocean

      Note that we are running as root inside the container and we are mounting the Docker
        socket into the container so that Jenkins can access Docker from within the container.

      Data produced and used by Jenkins will be stored in the Docker volume, 'jenkins-data'.


    3. Get the initial admin password automatically generated by Jenkins.

         $ docker container exec jenkins cat /var/jenkins_home/secrets/initialAdminPassword

         90e41fc304bc48d19d4a7507c8d2dcf0


    4. In the web browser, navigate to http://localhost:8080 to access the graphical UI of 
         Jenkins.

       Unlock Jenkins with the admin password retrieved in the previous step.


    5. Next, choose 'Install Suggested Plugins' to have Jenkins install the most useful plugins.

       Then, create an admin user.

           u: admin
           p: admin123
           e: admin@admin.com

       Restart the server if prompted.


    6. Click 'New Item' > 'Pipeline' and create a new pipeline.

         Name: sample-pipeline

         Pipeline Tab > Script: Sample Jenkinsfile from above


    7. Now, click 'Build Now' for the new pipeline in the main Jenkins menu.



- Using Docker to Power a CI/CD Pipeline - Integrating the Sample Application

    1. Initialize our project folder as a Git project.

         $ cd jenkins-pipeline && git init


    2. Add a 'package.json' file to the folder.

         {
           "name": "jenkins-pipeline",
           "version": "1.0.0",
           "main": "server.js",
           "scripts": {
             "start": "node server.js",
             "test": "jasmine"
           },
           "dependencies": {
             "express": "^4.17.1"
           },
           "devDependencies": {
             "jasmine": "^3.4.0"
           }
         }


    3. Add a 'hobbies.js' file, which implements logic to retrieve hobbies as a JavaScript 
         modules called 'hobbies'.  This file simulates a database by serving pre-canned
         data.

         # File: hobbies.js

         const hobbies = ["jogging","cooking","diving","swimming","reading"];

         exports.getHobbies = () => {
             return hobbies;
         }
         
         exports.getHobby = id => {
             if(id<1 || id > hobbies.length)
                 return null;
             return hobbies[id-1];
         }


    4. Next, add a 'server.js' file that defines a RESTful endpoint with 3 endpoints:

         GET /
         GET /hobbies
         GET /hobbies/:id


         # File: server.js

         const hobbies = require('./hobbies');
         const express = require('express');
         const app = express();
         
         app.listen(3000, '0.0.0.0', () => {
             console.log('Application listening at 0.0.0.0:3000');
         })
         
         app.get('/', (req, res) => {
             res.send('Sample API');
         })
         
         app.get('/hobbies', async (req, res) => {
             res.send(hobbies.getHobbies());
         })
         
         app.get('/hobbies/:id', async (req, res) => {
             const id = req.params.id;
             const hobby = hobbies.getHobby(id);
             if(!hobby){
                 res.status(404).send("Hobby not found");
                 return;
             }
             res.send();
         })


    5. Now, create a 'spec' subfolder and add some unit tests.

         # File: spec/hobbies-spec.js

         const hobbies = require('../hobbies');

         describe("API unit test suite", () => {
             describe("getHobbies", () => {
                 const list = hobbies.getHobbies();
                 it("returns 5 hobbies", () => {
                     expect(list.length).toEqual(5);
                 });
                 it("returns 'jogging' as first hobby", () => {
                     expect(list[0]).toBe("jogging");
                 });
             })
         })


    6. Add a 'spec/support' folder and add a Jasmine config file.

         # File: spec/support/jasmine.json

         {
             "spec_dir": "spec",
             "spec_files": [
               "**/*[sS]pec.js"
             ],
             "stopSpecOnExpectationFailure": false,
             "random": false
         }



- Using Docker to Power a CI/CD Pipeline - Pulling From Github

    1. Commit the code created locally.

         $ git add -A && git commit -m "First commit"


    2. Add a .gitignore file to avoid pushing the node_modules folder to Github.

         # File: .gitignore

         node_modules


    3. Log into your account on GitHub and create a new repository called 'jenkins-pipeline'.

       After the repo has been created, execute these commands:

           $ git remote add origin https://github.com/gnschenker/jenkins-pipeline.git
           $ git push -u origin master


    4. Now, we need to go into Jenkins and modify the configuration for the account.

         Main Menu > Configure > Pipeline

           SCM:  Git
           Repository URL:  /home/fod/ch07/jenkins-pipeline
           Credentials:  None
           Branches to Build:  */master
           Script Path:  Jenkinsfile

       Here, we configure Jenkins to pull code from GitHub and use a Jenkinsfile to define
         the pipeline.



- Using Docker to Power a CI/CD Pipeline - Creating the Jenkinsfile

    - We have defined that the Jenkinsfile must be in the project root folder.  This is the
         foundation of 'Pipeline-as-Code', since we will also commit the Jenkinsfile to Github.

         # File: Jenkinsfile

         pipeline {
            environment {
                registry = "gnschenker/jenkins-docker-test"
                DOCKER_PWD = credentials('docker-login-pwd')
            }
            agent {
                docker {
                    image 'gnschenker/node-docker'
                    args '-p 3000:3000'
                    args '-w /app'
                    args '-v /var/run/docker.sock:/var/run/docker.sock'
                }
            }
            options {
                skipStagesAfterUnstable()
            }
            stages {
                stage("Build"){
                    steps {
                        sh 'npm install'
                    }
                }
                stage("Test"){
                    steps {
                        sh 'npm test'
                    }
                }
                stage("Build & Push Docker image") {
                    steps {
                        sh 'docker image build -t $registry:$BUILD_NUMBER .'
                        sh 'docker login -u gnschenker -p $DOCKER_PWD'
                        sh 'docker image push $registry:$BUILD_NUMBER'
                        sh "docker image rm $registry:$BUILD_NUMBER"
                    }
                }
            }
        }


    - Going through this script piece by piece, here we define 2 environment variables 
        that will be available for all stages.

        environment {
            registry = "gnschenker/jenkins-docker-test"
            DOCKER_PWD = credentials('docker-login-pwd')
        }

      The 'registry' variable contains the full name of the image we will eventually produce
        and push to Docker Hub.

      The 'DOCKER_PWD' variable is the password to our Docker Hub account.  We are using the
        Jenkins 'credentials' function, which gives us access to a secret stored under the
        name 'docker-login-pwd' in Jenkins.


    - Next, we define the agent we want to run the Jenkins pipeline on.  In our case, we are
        using a Docker image, the 'gnschenker/node-docker' image.

      This image is based on 'node:12.10-alpine', with docker and curl installed.

        agent {
            docker {
                image 'gnschenker/node-docker'
                args '-v /var/run/docker.sock:/var/run/docker.sock'
            }
        }


    - Here, we build and test our application:

        stage("Build"){
            steps {
                sh 'npm install'
            }
        }
        stage("Test"){
            steps {
                sh 'npm test'
            }
        }


    - In our 'Build & Push Docker image' stage, after we have successfully built and tested 
        our application, we can create a Docker image for it and push it to a registry.

        stage("Build & Push Docker image") {
            steps {
                sh 'docker image build -t $registry:$BUILD_NUMBER .'
                sh 'docker login -u gnschenker -p $DOCKER_PWD'
                sh 'docker image push $registry:$BUILD_NUMBER'
                sh "docker image rm $registry:$BUILD_NUMBER"
            }
        }

        1. We use Docker to build the image.  The $BUILD_NUMBER variable is defined by Jenkins.
        2. We lock into Docker Hub.
        3. We push the image to Docker Hub.
        4. We remove the local copy of the image to save space, since it's in the registry now.


    - Remember that all build stages happen inside of our 'gnschenker/node-docker' builder
        container.  So, we're running Docker inside Docker.  This is why we mapped the Docker
        socket into the builder.


    - Let's add 2 more stages to the pipeline.  First, we'll add the 'Deploy and smoke test'
        stage.

        stage('Deploy and smoke test') {
            steps{
                sh './jenkins/scripts/deploy.sh'
            }
        }


    - Here is the script we will run in this stage, which we put in the 'jenkins/scripts' 
        subfolder of the project.

        # File: jenkins-pipeline/jenkins/scripts/deploy.sh

        #!/usr/bin/env sh

        echo "Removing api container if it exists..."
        docker container rm -f api || true
        echo "Removing network test-net if it exists..."
        docker network rm test-net || true
        
        echo "Deploying app ($registry:$BUILD_NUMBER)..."
        docker network create test-net
        
        docker container run -d \
            --name api \
            --net test-net \
            $registry:$BUILD_NUMBER
        
        # Logic to wait for the api component to be ready on port 3000
        
        read -d '' wait_for << EOF
        echo "Waiting for API to listen on port 3000..."
        while ! nc -z api 3000; do 
          sleep 0.1 # wait for 1/10 of the second before check again
          printf "."
        done
        echo "API ready on port 3000!"
        EOF
        
        docker container run --rm \
            --net test-net \
            node:12.10-alpine sh -c "$wait_for"
        
        echo "Smoke tests..."
        docker container run --name tester \
            --rm \
            --net test-net \
            gnschenker/node-docker sh -c "curl api:3000"


    - Finally, we'll add a 'Cleanup' step.

        stage('Cleanup') {
            steps{
                sh './jenkins/scripts/cleanup.sh'
            }
        }


      Here is that script:

        # File: jenkins-pipeline/jenkins/scripts/cleanup.sh

        #!/usr/bin/env sh
        docker rm -f api
        docker network rm test-net


    - Our final steps are:

        1. Checkout From Source Control
        2. Build
        3. Test
        4. Build and Push Docker Image
        5. Deploy and Smoke Test
        6. Cleanup


    - Now, we can commit our changes.

        $ git add --all && git commit -m "Defined code based pipeline"
        $ git push origin master

      If we click 'Build now' on the Jenkins main page, all of our steps should now run.