-----------------------------------------------------------
CHAPTER 4 - CREATING & MANAGING CONTAINER IMAGES
-----------------------------------------------------------

- What Are Images?

    - In Linux, everything is a file.  The whole OS is basically a filesystem with files and
        folders stored on the local disk.  

      An 'image' is basically a big tarball containing a layered filesystem.



- The Layered Filesystem

    - Container images are templates from which containers are created.  They are not just one
        monolithic block, but are rather composed of many layers.

                     Layer n
                     Layer n-1
                     ...
                     Layer 2
                     Layer 1 (Base Layer)


    - Each individual layer contains files and folders.  Each layer contains only the
        filesystem with respect to the underlying layers.  Docker then uses a union 
        filesystem to create a virtual filesystem out of the set of layers.

      A storage driver handles the details regarding the way these layers interact with
        each other.  Different storage drivers are available that have advantages and
        disadvantages in different situations.


    - The layers of a container image are all immutable.  Once they are generated, they
        cannot be changed.  The only possible operation affecting a layer is the 
        physical deletion of it.

      Since the layers are immuatable, they can be cached without ever becoming stale, 
        which leads to big performance advantages.


    - Here is an example image:

        3. Add static files (ie HTML, CSS, JS files)
        2. Add Nginx
        1. Alpine Linux

      Each layer contains only the delta of changes in regard to the previous set of
        layers.  The content of each layer is mapped to a special folder on the host
        system (usually a subdirectory of /var/lib/docker).


    - The base image is typically one of the official images found on Docker Hub, like
        Alpine, Ubuntu, or CentOS.  It is also possible to create an image from scratch.

      Docker Hub is a public registry for container images.  It is a central hub ideally
        suited for sharing public container images.



- The Writable Container Layer

    - When the Docker engine creates a container from an image, it adds a writable container
        layer on top of the stack of immuatable layers.

        Container Layer    r/w
        3. Add static files
        2. Add Nginx
        1. Alpine Linux


    - Another advantage of the layered approach is that image layers can be shared among many
        containers created from an image.  All that is needed is a thin, writable container
        layer for each container.

        Container1          Container2         Container3
        Thin r/w layer      Thin r/w layer     Thin r/w layer
                   \              |                /
                    \             |               /
                     \            |              /
                         3. Add static files
                         2. Add Nginx
                         1. Alpine Linux



- Copy-on-Write

    - Docker uses a copy-on-write technique when dealing with images.  If a layer needs a 
        file or folder on a lower layer, it just uses it.  If a layer needs to write to 
        a file in a lower layer, it first copies the file to the target layer, then
        modifies it.



- Graph Drivers

    - Graph drivers (aka storage drivers) are what enable the union filesystem.  A graph
        driver consolidates the multiple image layers into a root filesystem for the mount
        namespace of the container.

      To put it differently, the driver controls how images and containers are stored and
        managed on the Docker host.


    - Docker supports several different graph drivers using a pluggable architecture.  The
        preferred driver is 'overlay2', followed by 'overlay'.



- Creating Images

    - There are 3 ways to create a new container image on your system:

        1. Interactively build a container that contains all the additions and changes
             one desires and then commit those changes onto a new image.

        2. Use a Dockerfile to describe what's in the new image and then build this image
             using the Dockerfile as a manifest (this way is the most important).

        3. Import it into the system from a tarball.



- Interactive Image Creation

    - To create an image interactively, we start with a base image we want to use as a
        template and run it as a container interactively.

        # Run an alpine container interactively
        $ docker container run -it --name sample alpine bin/sh


        # Install ping command on image
        $ apk update && apk add iputils

        # Now we can use ping
        $ ping 127.0.0.1


    - Now, we can quit the container ('exit') and list all the containers on the system:

        # Container has exited
        $ docker container ls -a | grep sample

        eff7c92a1b98    alpine    "/bin/sh"    2 minutes ago      Exited (0) ...


    - To see what has changed in our container in relation to the base image, we can use the
        'container diff' command.

        # See changes from base image
        $ docker container diff sample

        # A for added, C for changed
        C /bin
        C /bin/ping
        C /bin/ping6
        A /bin/traceroute6


    - We can now use the 'container commit' command to persist the changes and create a
        new image from them.

        # Create new image 'my-alpine'
        $ docker container commit sample my-alpine


        # See new image on the system 
        #   (note that 'latest' is default tag)
        $ docker image ls

        REPOSITORY TAG      IMAGE ID        CREATED               SIZE
        my-alpine  latest   44bca4141130    About a minute ago    5.64MB
        ...


    - To get the list of layers our image consists of:

        # Print list of layers
        $ docker image history my-alpine



- Using Dockerfiles

    - Dockerfiles provide a declarative way of building images.  For instance, here is a
        Dockerfile used to containerize a Python 2.7 application.

        # Dockerfile
        FROM python:2.7                         # Layer 1 (Base Layer)
        RUN mkdir -p /app                       # Layer 2
        WORKDIR /app                            # Layer 3
        COPY ./requirements.txt /app/           # Layer 4
        RUN pip install -r requirements.txt     # Layer 5
        CMD ["python", "main.py"]               # Layer 6


      Each line of the Dockerfile results in a layer in the resulting image.  



- The FROM Keyword

    - Every Dockerfile starts with the 'FROM' keyword.  With it, we define which base image
        we want to start building our custom image from.

        # Build starting from Centos 7
        FROM centos:7

        # Build starting from official Python 2.7 image
        FROM python:2.7

        # Build starting from scratch
        FROM scratch



- The RUN Keyword

    - The argument for the 'RUN' keyword is any valid Linux command.

        # Install wget on Centos
        RUN yum install -y wget

        # Install wget on Ubuntu
        RUN apt-get update && apt-get install -y wget


        # Make a directory and navigate to it
        RUN mkdir -p /app && cd /app

        # Untar a file to new directory
        RUN tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz


        # We can also use multiline Linux commands
        RUN apt-get update \
          && apt-get install -y --no-install-recommends \
            ca-certificates \
            libexpat1 \
            libffi6 \
            libgdbm3 \
            libreadline7 \
            libsqlite3-0 \
            libssl1.1 \
          && rm -rf /var/lib/apt/lists/*



- The COPY and ADD Keywords

    - The 'COPY' and 'ADD' keywords are used to add some content to a base image.  Most 
        of the time, these are source files (ie from a web application) or binaries
        (from a compiled application).


    - Both keywords are used to copy files and folders from the host onto the image that 
        we're building.  They are very similar, except ADD also lets us copy and unpack
        TAR files, as well as provide a URL as a source for files and folders to copy.


        # Copy all files and folders in current dir to /app folder in container image
        COPY . /app

        # Copy everything in /web subfolder to /app/web in container image
        COPY ./web /app/web

        # Copy a single file into target folder and rename it
        COPY sample.txt /data/my-sample.txt


        # Unpack the tar file to the target folder
        ADD sample.tar /app/bin/

        # Copy remote file into target file
        ADD http://example.com/sample.txt /data/


    - Wildcards are allowed in the source path.

        # Copy all files starting with 'sample'
        COPY ./sample* /mydir/


    - By default, all files and folders inside the image will have a UID and GID of 0.
        For both the ADD and COPY commands, we can change the ownership using the 
        '--chown' flag.

        # Assigns UID of 11, GID of 22 to files added
        ADD --chown=11:22 ./data/files* /app/data/



- The WORKDIR Keyword

    - The 'WORKDIR' keyword defines the working dirctory or context that is used when a 
        container is run from our custom image.  All activity that happens after this
        statement will use this directory as the working directory.


        # This creates a file in /app/bin
        RUN cd /app/bin
        RUN touch sample.txt

        # Same, but sets the context across the layers of the image
        WORKDIR /app/bin
        RUN touch sample.txt



- The CMD and ENTRYPOINT Keywords

    - The CMD and ENTRYPOINT keywords are special.  While all other keywords defined for a
        Dockerfile are executed at the time the image is built by the Docker builder, 
        these keywords are actually deifinitions of what will happen when the container is
        started.

      When the container runtime starts a container, it needs to know what the process or
        application is that will be run inside the container.  CMD and ENTRYPOINT are used
        to tell Docker what the start process is and how to start that process.


    - To understand 2 keywords, look at this linux command:

        # Linux command
        $ ping 8.8.8.8 -c 3


      The ENTRYPOINT keyword defines the command to be run and the CMD keyword defines the
        parameters for the command.

        # Dockerfile
        FROM alpine:latest
        ENTRYPOINT ["ping"]
        CMD ["8.8.8.8", "-c", 3]


    - For both ENTRYPOINT and CMD, the values are formatted as a JSON array of strings,
        where the individual items correspond to the tokens of the expressions (separated
        by whitespace).  This is called the 'exec form', and is the preferred way of
        dealing with ENTRYPOINT and CMD.

      Alternatively, we can use the 'shell form':

          CMD command param1 param2


    - Now we can build an image from the Dockerfile we just created and run it:

        # Build pinger image
        $ docker image build -t pinger .

        # Run container from pinger image
        $ docker container run --rm -it pinger


    - To override the CMD and ENTRYPOINT options specified:

        # Override the CMD arguments from the Dockerfile
        $ docker container run --rm -it pinger -w 5 127.0.0.1

        # Override the ENTRYPOINT
        $ docker container run --rm -it --entrypoint /bin/sh pinger


    - Note that again, we can just put the entire command into a CMD statement, but this is
        not recommended.

        # Put entire command expression in CMD
        FROM alpine:latest
        CMD wget -O - http://www.google.com



- A Complex Dockerfile

    -  Now, we can put these ideas together to create a more complex Dockerfile:

        # Dockerfile
        FROM node:9.4                   # Building image for Node application
        RUN mkdir -p /app               # Create /app folder in image filesystem
        WORKDIR /app                    # Defines new folder as working directory
        COPY package.json /app/         # Copy package.json files into /app folder
        RUN npm install                 # Run npm install using package.json
        COPY . /app                     # Now that Node dependencies are installed, copy app files
        ENTRYPOINT ["npm"]              # Run 'npm start' when container is run
        CMD ["start"]



- Building an Image

    - In our home directory, we create a 'FundamentalsOfDocker' subdirectory.

        # Create subdirectory
        $ mkdir ~/FundamentalsOfDocker
        $ cd ~/FundamentalsOfDocker


    - Now, create a 'sample1' subfolder in that directory.

        # Create subdirectory
        $ mkdir sample1 && cd sample1


    - In the ~/FundamentalsOfDocker/sample1 directory, create a file called 'Dockerfile'.

        # Create Dockerfile
        $ vim Dockerfile

        # File contents
        FROM centos:7
        RUN yum install -y wget


    - Now, build a new container image using the Dockerfile.

        # Build image 
        #   (with Dockerfile location specified)
        #   (last argument is directory where Dockerfile is located)
        $ docker image build -t my-centos -f Dockerfile .

        # Build image 
        #   (automatically looks for file 'Dockerfile')
        $ docker image build -t my-centos .


      Looking at the output of this command:

        - The first thing the builder does is package the files in the current build
            context (excluding the files and folders specified in the .dockerignore)

            Sending build context to Docker daemon 2.048kB
            ...

        - Next, step 1/2 creates the base image:

            Step 1/2 : FROM centos:7
            7: Pulling from library/centos
            af4b0a2388c6: Pull complete
            Digest: sha256:2671f7a...
            Status: Downloaded newer image for centos:7
            ---> ff426288ea90
            ...

        - Next, step 2/2 installs wget, and the previous container created after step 1
            is discarded:

            Step 2/2 : RUN yum install -y wget
            ---> Running in bb726903820c
            ...
            ...
            Removing intermediate container bb726903820c
            ---> bc070cc81b87

        - Finally, we are given the ID for the complete image, which has the tag 
            'my-centos:latest':

            Successfully built bc070cc81b87
            Successfully tagged my-centos:latest



- Multistep Builds

    - In this example, we have a simple C program:

        File: hello.c

        #include <stdio.h>
        int main (void)
        {
            printf ("Hello, world!\n");
            return 0;
        }


    - Now, we'll write a Dockerfile to build and run our application.  Note that in this case,
        we are copying all artifacts into the image.

        File: Dockerfile

        FROM alpine:3.7
        RUN apk update && apk add --update alpine-sdk
        RUN mkdir /app
        WORKDIR /app
        COPY . /app
        RUN mkdir bin
        RUN gcc -Wall hello.c -o bin/hello
        CMD /app/bin/hello


    - Now, we build the image:

        # Build image
        $ docker image build -t hello-world .

        # See the new image
        $ docker image ls | grep hello-world


    - Instead of including all artifacts in the image, we can use a multistep Dockerfile.

        File: Dockerfile

        FROM alpine:3.7 AS build
        RUN apk update && apk add --update alpine-sdk
        RUN mkdir /app
        WORKDIR /app
        COPY . /app
        RUN mkdir bin
        RUN gcc hello.c -o bin/hello
         
        FROM alpine:3.7
        COPY --from=build /app/bin/hello /app/hello
        CMD /app/hello


    - Note the difference in size between the images, since the 'hello-world-small' image
        doesn't include the alpine-sdk, just the compiled binary of the first step.

        $ docker image ls | grep hello-world

        hello-world-small  latest   f98...   20 seconds ago   4.16MB
        hello-world        latest   469...   10 minutes ago   176MB



- Dockerfile Best Practices

    - We need to always remember that containers are meant to be ephemeral.  A container can 
        be stopped and destroyed, and a new one can be built and put in its place with a
        minimum of setup and config.  We should try to keep setup and teardown times low.


    - We should order commands in the Dockerfile to use caching as much as possible.  For 
        instance, when we're rebuilding a previously built image, and one layer needs to
        be rebuilt, all the subsequent layers also need to be rebuilt.

        # 'npm install' gets re-run every time changes are made to /app
        FROM node:9.4
        RUN mkdir -p /app
        WORKIR /app
        COPY . /app
        RUN npm install
        CMD ["npm", "start"]

        # 'npm install' is not re-run every time
        FROM node:9.4
        RUN mkdir -p /app
        WORKIR /app
        COPY package.json /app/
        RUN npm install
        COPY . /app
        CMD ["npm", "start"]


    - It's also a good idea to keep your total number of layers low when possible.  The more
        layers there are, the more work the graph driver has to do to build a container.

        # Combine multiple statements into a single layer
        RUN apt-get update \
            && apt-get install -y ca-certificates \
            && rm -rf /var/lib/apt/lists/*


    - Reduce the image size by using a .dockerignore file.  Avoid copying unnecessary files
        and folders into an image.


    - Avoid installing unnecessary packages into the filesystem of the image.  Keep the image as
        lean as possible.


    - Use multi-stage builds so that the resulting image is as small as possible and only contains
        the absolute minimum  needed to run your application or service.



- Saving and Loading Images

    - The 3rd way to create a new container image is by importing or loading it from a file.
        A container image is nothing more than a tarball.

        # Export an existing image to a tarball
        $ docker image save -o ./backup/my-alpine.tar my-alpine


    - To load an image into the system from a tarball:

        # Load an image
        $ docker image load -i ./backup/my-alpine.tar



- Example - Containerizing a Legacy App

    - Usually, we find ourselves maintaining complex legacy apps with sparse documentation.
        Sometimes, we can use containers to make those apps easier to manage.


    - We should analyze external dependencies.  For instance, we should ask questions like:

        1. Does it use a database?  Which one?  What does the connection string look like?

        2. Does it use external APIs?  What are the API keys and key secrets?

        3. Is it consuming from or publishing to an Enterprise Service Bus?


    - Next, we want to locate all of the source code and other assets.  Ideally, they should be
        located in a single folder.  If they aren't, we can use the ADD and COPY commands to
        copy components from the various sources.

      Then, we need to find out how the application was built and packaged.  For instance, using
        Maven (Java), MSBuild (.NET), or make (C or C++).  


    - Applications need to be configured.  For instance, they have logging settings, connection
        strings to DBs, hostnames and URIs for services, etc.  We differentiate 3 types of 
        configurations:

        1. Build time   = Needed during building of application and docker image.

        2. Environment  = Varies depending on the environment in which the application is running
                            (ie DEVELOPMENT, STAGING, PRODUCTION).  These configurations are
                            applied at runtime.

        3. Runtime      = This is information the application retrieves during runtime, such as
                            secrets to access an external API.


    - Every mission-critical enterprise app needs to deal with secrets in one form or another.
        This includes connection information for databases and API keys.  

      There are many ways applications acquire secrets.  The worst way is to hard-code them in 
        plain text.  A better way is to access them from a secrets store that uses a secure
        connection like TLS.


    - Now, we can create the Dockerfile.  This will most likely require a lot of iterations and
        fine-tuning.  

        1. We need to choose a base image.  If an image for our application type like Jave is
             available, we should use that.  Otherwise, we should use a Linux distro image.

        2. We need to assemble sources using the COPY and ADD commands.

        3. We need to build the application.

             # Build application
             RUN mvn --clean install

             # Define environment variables
             ENV foo=bar
             ENV baz=123

             # Open ports
             EXPOSE 5000
             EXPOSE 15672/tcp

        4. We start the application.

             # Start application
             ENTRYPOINT java -jar pet-shop.war


    - We might wonder what we gain by bothering to containerize our legacy app.  Studies
        have shown we can cut our maintenance costs in half and our time to release by
        90%.  



- Sharing or Shipping Images

    - In order to ship our custom image to other environments, we need to:

        1. Give it a globally unique name (tag the image)
        2. Publish the image to an image registry



- Tagging an Image

    - Each image has a tag.  It is used to version images, and it has a broader reach than
        just being a version number.

        # Get alpine:latest
        $ docker image pull alpine

        # Get an older version
        $ docker image pull alpine:3.5



- Image Namespaces

    - So far, we haven't thought too much about where we are pulling images from.  Our docker
        environment is configured so that all images are pulled from DockerHub by default.
        We pulled official images like 'alpine' or 'busybox'.


    - Now, we will learn about how images are namespaced.  The fully qualified image name 
        looks like:

        <registry URL>/<User or Org>/<name>:<tag>


        <registry URL>
          Is 'docker.io' by default, but there are also popular registries at Google, AWS, 
            Azure, Red Hat, and Artifactory.  If omitted, Docker Hub is chosen.

        <User or Org>
          Private ID of individual or organization defined on DockerHub or other registry.
            If the image is an 'official image' on Docker Hub, no user or organization
            namespace is needed.

        <name>
          This the name of the image.

        <tag>
          This is the tag of the image.  If it is omitted, 'latest' is used.



- Official Images

    - Official repositories are repositories hosted on Docker Hub that are curated by 
        organizations that are also responsible for the software packaged in the image.

      Official images are meant to provide base OS repose, images for programming language
        runtimes, frequently used data storage, and other important services.



- Pushing Images to a Registry

    - Eventually, we will want to ship our images to a target environment, such as test, QA, or
        a production system.  We usually use a container registry for this.  

      Docker Hub is the most popular registry.  A personal account like 'jsmith' is fine for
        personal projects, while an organization would want to have an account like 'acme'.


    - Let's say we want to push the latest version of alpine to our account and give it a version
        number 1.0.

        # Tag the image
        $ docker image tag alpine:latest gnschenker/alpine:1.0

        # Log into Docker Hub account
        $ docker login -u jsmith -p <my secret pw>

        # Push the image
        $ docker image push jsmith/alpine:1.0


    - For each image we push, we automatically create a repository, which can be public or 
        private.